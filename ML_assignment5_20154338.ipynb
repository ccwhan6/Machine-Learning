{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to install pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "transform = transforms.Compose([#transforms.Resize((256,256)),  \n",
    "                                transforms.Grayscale(),\t\t# the code transforms.Graysclae() is for changing the size [3,100,100] to [1, 100, 100] (notice : [channel, height, width] )\n",
    "                                transforms.ToTensor(),])\n",
    "\n",
    "\n",
    "#train_data_path = 'relative path of training data set'\n",
    "train_data_path = './horse-or-human/horse-or-human/train'\n",
    "trainset = torchvision.datasets.ImageFolder(root=train_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "# if shuffle=True, the data reshuffled at every epoch \n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=3, shuffle=False, num_workers=1)  \n",
    "\n",
    "\n",
    "validation_data_path = './horse-or-human/horse-or-human/validation'\n",
    "valset = torchvision.datasets.ImageFolder(root=validation_data_path, transform=transform)\n",
    "# change the valuse of batch_size, num_workers for your program\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=3, shuffle=False, num_workers=1)\n",
    "TTOTAL = 1027\n",
    "VTOTAL = 256\n",
    "train_data = np.zeros((TTOTAL,10000))\n",
    "val_data = np.zeros((VTOTAL,10000))\n",
    "train_lab = np.zeros((TTOTAL,1))\n",
    "val_lab = np.zeros((VTOTAL,1))\n",
    "\n",
    "\n",
    "NUM_EPOCH = 1\n",
    "index = 0\n",
    "for epoch in range(0, NUM_EPOCH):\n",
    "    # load training images of the batch size for every iteration\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs)\n",
    "        #print(np.concatenate((temp[:1, :],labels[0]),axis = 0))\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        lab_temp = labels.numpy() \n",
    "        for num in range(0, len(inputs)):\n",
    "            temp = inputs.view(len(inputs), -1)\n",
    "            temp = temp[num:num+1, :].numpy()\n",
    "            train_data[index] = temp[0]\n",
    "            train_lab[index] = lab_temp[num]\n",
    "            index += 1                                 #make torch to numpy and save it with vectorize (train data)\n",
    "\n",
    "\n",
    "    index = 0\n",
    "\n",
    "    # load validation images of the batch size for every iteration\n",
    "    for i, data in enumerate(valloader):\n",
    "        \n",
    "        # inputs is the image\n",
    "        # labels is the class of the image\n",
    "        inputs, labels = data\n",
    "\n",
    "        # if you don't change the image size, it will be [batch_size, 1, 100, 100]\n",
    "        #print(inputs)\n",
    "\n",
    "        # if labels is horse it returns tensor[0,0,0] else it returns tensor[1,1,1]\n",
    "        #print(labels)    \n",
    "        \n",
    "        lab_temp = labels.numpy()\n",
    "        for num in range(0, len(inputs)):\n",
    "                temp = inputs.view(len(inputs), -1)\n",
    "                temp = temp[num:num+1, :].numpy()\n",
    "                val_data[index] = temp[0]\n",
    "                val_lab[index] = lab_temp[num]\n",
    "                index += 1                             #make torch to numpy and save it with vectorize (validation data)\n",
    "\n",
    "train_data = np.concatenate((train_data, np.ones((TTOTAL,1))), axis = 1)\n",
    "val_data = np.concatenate((val_data, np.ones((VTOTAL,1))), axis = 1)           #attach bias to data set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## methods for iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def tanh(z):\n",
    "    return (np.exp(z) - np. exp(-z)) / (np.exp(z) + np.exp(-z))\n",
    "\n",
    "def ReLU(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def L_ReLU(z, alpha):\n",
    "    return np.maximum(alpha * z, z)\n",
    "    \n",
    "def d_R(z):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z <= 0] = 0\n",
    "    return dz\n",
    "\n",
    "def d_LR(z, alpha):\n",
    "    dz = np.ones_like(z)\n",
    "    dz[z <= 0] = alpha\n",
    "    return dz\n",
    "\n",
    "def crossEntropy(l,y):\n",
    "    return -((l * np.log(y)) + ((1 - l) * np.log(1 - y)))      \n",
    "\n",
    "def loss(l,y, total):\n",
    "    return (1 / total) * (np.sum(crossEntropy(l,y)))\n",
    "\n",
    "def accuracy(l,y, total):\n",
    "    cor = 0\n",
    "    for num in range(total):\n",
    "        if (y[num] > 0.5 and l[num] == 1):\n",
    "            cor = cor + 1\n",
    "        elif (y[num] <= 0.5 and l[num] == 0):\n",
    "            cor = cor + 1\n",
    "    return cor / total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make iteration with Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "H1 = 128\n",
    "H2 = 32\n",
    "\n",
    "u = np.random.randn(H1, 10001)/np.sqrt(10001/2)\n",
    "v = np.random.randn(H2, H1)/np.sqrt(H1/2)\n",
    "w = np.random.randn(1, H2)/np.sqrt(H2/2)\n",
    "\n",
    "alpha = 0.01\n",
    "learning_rate = 0.01\n",
    "train_loss_LR = []\n",
    "val_loss_LR = []\n",
    "train_acc_LR = []\n",
    "val_acc_LR = []\n",
    "last = 0\n",
    "end = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # forward propagation \n",
    "    a = np.dot(u, train_data.T)\n",
    "    y = L_ReLU(a, alpha)\n",
    "    b = np.dot(v, y)\n",
    "    z = L_ReLU(b, alpha)\n",
    "    c = np.dot(w, z)\n",
    "    h = sigmoid(c)\n",
    "    h_v = sigmoid(np.dot(w,L_ReLU(np.dot(v,L_ReLU(np.dot(u, val_data.T), alpha)),alpha)))\n",
    "    \n",
    "    # back propagation \n",
    "    temp = h.T - train_lab\n",
    "    w_temp = np.dot(temp.T, z.T) * (1 / TTOTAL)\n",
    "\n",
    "    temp = np.dot(temp, w) * (d_LR(z, alpha)).T\n",
    "    v_temp = (np.dot(temp.T, y.T) * (1 / TTOTAL))\n",
    "\n",
    "    u_temp = (np.dot((np.dot(temp, v) * (d_LR(y, alpha)).T).T, train_data)) * (1 / TTOTAL)\n",
    "    \n",
    "    w = w - (learning_rate * w_temp)\n",
    "    v = v - (learning_rate * v_temp)\n",
    "    u = u - (learning_rate * u_temp)\n",
    "    \n",
    "    t1 = loss(train_lab, h.T, TTOTAL)\n",
    "    t2 = loss(val_lab, h_v.T, VTOTAL)\n",
    "    \n",
    "    # get loss and accuracy\n",
    "    train_loss_LR.append(t1)\n",
    "    val_loss_LR.append(t2)\n",
    "    \n",
    "    t1 = accuracy(train_lab, h.T, TTOTAL)\n",
    "    t2 = accuracy(val_lab, h_v.T, VTOTAL)\n",
    "    \n",
    "    train_acc_LR.append(t1)\n",
    "    val_acc_LR.append(t2)\n",
    "    \n",
    "    \n",
    "    if (abs(train_loss_LR[end]-last) < 10e-6):\n",
    "        break\n",
    "    else :\n",
    "        last = train_loss_LR[end]\n",
    "        end = end + 1\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot loss, accuracy, and calculation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = [np.argmin(train_loss_LR),np.argmin(val_loss_LR)]\n",
    "y = [train_loss_LR[x[0]], val_loss_LR[x[1]]]\n",
    "\n",
    "\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(train_loss_LR)\n",
    "plt.plot(val_loss_LR)\n",
    "plt.scatter(x,y, color = 'red')\n",
    "plt.legend(['train_loss','val_loss'])\n",
    "plt.show()\n",
    "\n",
    "x = [np.argmax(train_acc_LR),np.argmax(val_acc_LR)]\n",
    "y = [train_acc_LR[x[0]], val_acc_LR[x[1]]]\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(train_acc_LR)\n",
    "plt.plot(val_acc_LR)\n",
    "plt.scatter(x,y, color = 'red')\n",
    "plt.title(\"Accuracy\")\n",
    "plt.legend(['train_acc','val_acc'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final loss and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  final      |       loss        |      accuracy       | \n",
      "-------------------------------------------------------- \n",
      "  training   |    0.0477202680   |    0.9970788705     | \n",
      "-------------------------------------------------------- \n",
      " validation  |    0.4199980220   |    0.8710937500     | \n",
      "-------------------------------------------------------- \n",
      "  best       |       loss        |      accuracy       | \n",
      "-------------------------------------------------------- \n",
      "  training   |    0.0507438324   |    0.9980525803     | \n",
      "-------------------------------------------------------- \n",
      " validation  |    0.3475455954   |    0.9062500000     | \n",
      "-------------------------------------------------------- \n"
     ]
    }
   ],
   "source": [
    "print(\"  final      |       loss        |      accuracy       | \")\n",
    "print(\"-------------------------------------------------------- \")\n",
    "print(\"  training   |    %.10f   |    %.10f     | \" %(train_loss_LR[-1],train_acc_LR[-1]))\n",
    "print(\"-------------------------------------------------------- \")\n",
    "print(\" validation  |    %.10f   |    %.10f     | \" %(val_loss_LR[-1],val_acc_LR[-1]))\n",
    "print(\"-------------------------------------------------------- \")\n",
    "print(\"  best       |       loss        |      accuracy       | \")\n",
    "print(\"-------------------------------------------------------- \")\n",
    "print(\"  training   |    %.10f   |    %.10f     | \" %(train_loss_LR[np.argmax(train_acc_LR)],train_acc_LR[np.argmax(train_acc_LR)] ))\n",
    "print(\"-------------------------------------------------------- \")\n",
    "print(\" validation  |    %.10f   |    %.10f     | \" %(val_loss_LR[np.argmax(val_acc_LR)],val_acc_LR[np.argmax(val_acc_LR)]))\n",
    "print(\"-------------------------------------------------------- \")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
